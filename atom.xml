<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zhaoyupeng_blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-05-25T15:45:26.838Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>zhaoyupeng</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>vcpkg</title>
    <link href="http://example.com/2022/05/25/vcpkg/"/>
    <id>http://example.com/2022/05/25/vcpkg/</id>
    <published>2022-05-25T15:35:56.000Z</published>
    <updated>2022-05-25T15:45:26.838Z</updated>
    
    <content type="html"><![CDATA[<p>Install vcpkg</p><p>Installing vcpkg is a two-step process: first, clone the repo, then run the bootstrapping script to produce the vcpkg binary. The repo can be cloned anywhere, and will include the vcpkg binary after bootstrapping as well as any libraries that are installed from the command line. It is recommended to clone vcpkg as a submodule for CMake projects, but to install it globally for MSBuild projects. If installing globally, we recommend a short install path like: C:\src\vcpkg or C:\dev\vcpkg, since otherwise you may run into path issues for some port build systems.</p><p><strong>Step 1: Clone the vcpkg repo</strong></p><p>git clone <a href="https://github.com/Microsoft/vcpkg.git">https://github.com/Microsoft/vcpkg.git</a></p><p>Make sure you are in the directory you want the tool installed to before doing this.</p><p><strong>Step 2: Run the bootstrap script to build vcpkg</strong></p><p>.\vcpkg\bootstrap-vcpkg.bat</p><p>Install libraries for your project</p><p>vcpkg install [packages to install]</p><p>Using vcpkg with MSBuild &#x2F; Visual Studio (may require elevation)</p><p>vcpkg integrate install</p><p>After this, you can create a new project or open an existing one in the IDE. All installed libraries should already be discoverable by IntelliSense and usable in code without additional configuration.</p><p>Using vcpkg with CMake</p><p>In order to use vcpkg with CMake outside of an IDE, you can use the toolchain file:</p><p>cmake -B [build directory] -S . -DCMAKE_TOOLCHAIN_FILE&#x3D;[path to vcpkg]&#x2F;scripts&#x2F;buildsystems&#x2F;vcpkg.cmake</p><p>Then build with:</p><p>cmake –build [build directory]</p><p>With CMake, you will need to use find_package() to reference the libraries in your Cmakelists.txt files.</p><p>Install vcpkg</p><p>Installing vcpkg is a two-step process: first, clone the repo, then run the bootstrapping script to produce the vcpkg binary. The repo can be cloned anywhere, and will include the vcpkg binary after bootstrapping as well as any libraries that are installed from the command line. It is recommended to clone vcpkg as a submodule to an existing project if possible for greater flexibility.</p><p><strong>Step 1: Clone the vcpkg repo</strong></p><p>git clone <a href="https://github.com/Microsoft/vcpkg.git">https://github.com/Microsoft/vcpkg.git</a></p><p>Make sure you are in the directory you want the tool installed to before doing this.</p><p><strong>Step 2: Run the bootstrap script to build vcpkg</strong></p><p>.&#x2F;vcpkg&#x2F;bootstrap-vcpkg.sh</p><p>Install libraries for your project</p><p>vcpkg install [packages to install]</p><p>Using vcpkg with CMake</p><p>In order to use vcpkg with CMake outside of an IDE, you can use the toolchain file:</p><p>cmake -B [build directory] -S . -DCMAKE_TOOLCHAIN_FILE&#x3D;[path to vcpkg]&#x2F;scripts&#x2F;buildsystems&#x2F;vcpkg.cmake</p><p>Then build with:</p><p>cmake –build [build directory]</p><p>With CMake, you will need to find_package() to reference the libraries in your Cmakelists.txt files.</p><p>Install vcpkg</p><p>Installing vcpkg is a two-step process: first, clone the repo, then run the bootstrapping script to produce the vcpkg binary. The repo can be cloned anywhere, and will include the vcpkg binary after bootstrapping as well as any libraries that are installed from the command line. It is recommended to clone vcpkg as a submodule to an existing project if possible for greater flexibility.</p><p><strong>Step 1: Clone the vcpkg repo</strong></p><p>git clone <a href="https://github.com/Microsoft/vcpkg.git">https://github.com/Microsoft/vcpkg.git</a></p><p>Make sure you are in the directory you want the tool installed to before doing this.</p><p><strong>Step 2: Run the bootstrap script to build vcpkg</strong></p><p>.&#x2F;vcpkg&#x2F;bootstrap-vcpkg.sh</p><p>Install libraries for your project</p><p>vcpkg install [packages to install]</p><p>Using vcpkg with CMake</p><p>In order to use vcpkg with CMake outside of an IDE, you can use the toolchain file:</p><p>cmake -B [build directory] -S . -DCMAKE_TOOLCHAIN_FILE&#x3D;[path to vcpkg]&#x2F;scripts&#x2F;buildsystems&#x2F;vcpkg.cmake</p><p>Then build with:</p><p>cmake –build [build directory]</p><p>With CMake, you will need to find_package to reference the libraries in your Cmakelists.txt files.</p><p>Browse the <a href="https://vcpkg.io/en/docs/README.html">vcpkg documentation</a> to learn more about how to enable different workflows.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Install vcpkg&lt;/p&gt;
&lt;p&gt;Installing vcpkg is a two-step process: first, clone the repo, then run the bootstrapping script to produce the vcpk</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>CGI</title>
    <link href="http://example.com/2022/05/23/CGI/"/>
    <id>http://example.com/2022/05/23/CGI/</id>
    <published>2022-05-23T15:35:49.000Z</published>
    <updated>2022-05-23T15:38:28.629Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.jianshu.com/p/9a3122862b64">https://www.jianshu.com/p/9a3122862b64</a></p><h5 id="设置映射路径-“-x2F-usr-x2F-local-x2F-htdocs-x2F-cgi-bin-x2F-“为真实路径"><a href="#设置映射路径-“-x2F-usr-x2F-local-x2F-htdocs-x2F-cgi-bin-x2F-“为真实路径" class="headerlink" title="设置映射路径(“&#x2F;usr&#x2F;local&#x2F;htdocs&#x2F;cgi-bin&#x2F;“为真实路径)"></a>设置映射路径(“&#x2F;usr&#x2F;local&#x2F;htdocs&#x2F;cgi-bin&#x2F;“为真实路径)</h5><ul><li>注意这里 路径末尾一定要加&#x2F; ，否则apache是无法打到该路径下的文件的。</li></ul><blockquote><p>ScriptAlias &#x2F;cgi-bin&#x2F; “&#x2F;usr&#x2F;local&#x2F;htdocs&#x2F;cgi-bin&#x2F;“</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://www.jianshu.com/p/9a3122862b64&quot;&gt;https://www.jianshu.com/p/9a3122862b64&lt;/a&gt;&lt;/p&gt;
&lt;h5 id=&quot;设置映射路径-“-x2F-usr-x2F-local-x2F-ht</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>折半查找</title>
    <link href="http://example.com/2022/05/23/%E6%8A%98%E5%8D%8A%E6%9F%A5%E6%89%BE/"/>
    <id>http://example.com/2022/05/23/%E6%8A%98%E5%8D%8A%E6%9F%A5%E6%89%BE/</id>
    <published>2022-05-22T16:42:32.000Z</published>
    <updated>2022-05-22T16:43:24.451Z</updated>
    
    <content type="html"><![CDATA[<h1 id="查找算法-折半查找判定树及平均查找长度"><a href="#查找算法-折半查找判定树及平均查找长度" class="headerlink" title="查找算法-折半查找判定树及平均查找长度"></a>查找算法-折半查找判定树及平均查找长度</h1><p><a href="https://www.jianshu.com/u/de0ffb33f6be"><img src="https://cdn2.jianshu.io/assets/default_avatar/14-0651acff782e7a18653d7530d6b27661.jpg"></a></p><p><a href="https://www.jianshu.com/u/de0ffb33f6be">Jorunk</a>关注</p><p>2020.02.06 10:05:37字数 956阅读 2,295</p><p>从折半查找的过程看，以有序表的中间记录作为比较对象，并以中间记录将表分割为两个子表，对子表继续上述操作。所以，对表中每个记录的查找过程，可用二叉树来描述，二叉树中的每个结点对应有序表中的一个记录，结点中的值为该记录在表中的位置。通常称这个描述折半查找过程的二叉树为折半查找判定树。</p><p>长度为n的折半查找判定树的构造方法为：</p><p>⑴ 当n&#x3D;0时，折半查找判定树为空；</p><p>⑵ 当n＞0时，折半查找判定树的根结点是有序表中序号为mid&#x3D;(n+1)&#x2F;2的记录，根结点的左子树是与有序表r[1] ~ r[mid-1]相对应的折半查找判定树，根结点的右子树是与r[mid+1] ~ r[n]相对应的折半查找判定树。</p><p>例如，长度为10的折半查找判定树的具体生成过程为：</p><p>⑴ 在长度为10的有序表中进行折半查找，不论查找哪个记录，都必须先和中间记录进行比较，而中间记录的序号为(1+10)&#x2F;2&#x3D;5（注意是整除即向下取整），即判定树的根结点是5，如图7-2(a)所示；</p><p>⑵ 考虑判定树的左子树，即将查找区间调整到左半区，此时的查找区间是</p><p>[1，4]，也就是说，左分支上为根结点的值减1，代表查找区间的高端high，此时，根结点的左孩子是(1+4)&#x2F;2&#x3D;2，如图7-2(b)所示；</p><p>⑶ 考虑判定树的右子树，即将查找区间调整到右半区，此时的查找区间是</p><p>[6，10]，也就是说，右分支上为根结点的值加1，代表查找区间的低端low，此时，根结点的右孩子是(6+10)&#x2F;2&#x3D;8，如图7-2(c)所示；</p><p>⑷ 重复⑵⑶步，依次确定每个结点的左右孩子，如图7-2(d)所示。</p><p><img src="//upload-images.jianshu.io/upload_images/6565380-62d0a8117cc7a255?imageMogr2/auto-orient/strip%7CimageView2/2/w/239/format/webp"></p><p><img src="//upload-images.jianshu.io/upload_images/6565380-9a6d4c0f4bd8d175?imageMogr2/auto-orient/strip%7CimageView2/2/w/644/format/webp"></p><p>对于折半查找判定树，需要补充以下两点：<br>⑴ 折半查找判定树是一棵二叉排序树，即每个结点的值均大于其左子树上所有结点的值，小于其右子树上所有结点的值；<br>⑵ 折半查找判定树中的结点都是查找成功的情况，将每个结点的空指针指向一个实际上并不存在的结点——称为外结点，所有外结点即是查找不成功的情况，如图7-2(e)所示。如果有序表的长度为n，则外结点一定有n+1个。 在折半查找判定树中，某结点所在的层数即是查找该结点的比较次数，整个判定树代表的有序表的平均查找长度即为查找每个结点的比较次数之和除以有序表的长度。例如，长度为10的有序表的平均查找长度为：<br>ASL&#x3D;(1×1+2×2+3×4+4×3)&#x2F;10&#x3D;29&#x2F;10<br>在折半查找判定树中，查找不成功时的比较次数即是查找相应外结点时与内结点的比较次数。整个判定树代表的有序表在查找失败时的平均查找长度即为查找每个外结点的比较次数之和除以外结点的个数。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;查找算法-折半查找判定树及平均查找长度&quot;&gt;&lt;a href=&quot;#查找算法-折半查找判定树及平均查找长度&quot; class=&quot;headerlink&quot; title=&quot;查找算法-折半查找判定树及平均查找长度&quot;&gt;&lt;/a&gt;查找算法-折半查找判定树及平均查找长度&lt;/h1&gt;&lt;p&gt;&lt;a</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>AOE网络图</title>
    <link href="http://example.com/2022/05/11/AOE%E7%BD%91%E7%BB%9C%E5%9B%BE/"/>
    <id>http://example.com/2022/05/11/AOE%E7%BD%91%E7%BB%9C%E5%9B%BE/</id>
    <published>2022-05-11T09:59:25.000Z</published>
    <updated>2022-05-11T10:00:14.577Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/suncoolcat/">滴水穿石</a></p><h2 id="教你轻松计算AOE网关键路径"><a href="#教你轻松计算AOE网关键路径" class="headerlink" title="教你轻松计算AOE网关键路径"></a><a href="https://www.cnblogs.com/suncoolcat/p/3402527.html">教你轻松计算AOE网关键路径</a></h2><h1 id="认识AOE网"><a href="#认识AOE网" class="headerlink" title="认识AOE网"></a>认识AOE网</h1><p>有向图中，用顶点表示活动，用有向边表示活动之间开始的先后顺序，则称这种有向图为AOV网络；AOV网络可以反应任务完成的先后顺序（拓扑排序）。</p><p>在AOV网的边上加上权值表示完成该活动所需的时间，则称这样的AOV网为AOE网，如下图：</p><p><img src="//img-blog.csdn.net/20131101084129453?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2FuZzM3OTI3NTYxNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center"></p><p>图中，顶点表示事件（能被触发，两特征属性：最早发生时间Ve(j);最晚发生时间Vl(j)），边表示活动（能被开始，两特征属性：最早开始时间e(i)；最晚开始时间l(i)），权表示活动持续时间，通常用AOE网来估算工程完成的时间</p><p>两条原则：</p><p>Ø  只有某顶点所代表的事件发生后，从该顶点出发的各活动才能开始</p><p>Ø  只有进入某顶点的各活动都结束，该顶点所代表的事件才能发生</p><h1 id="计算关键路径"><a href="#计算关键路径" class="headerlink" title="计算关键路径"></a>计算关键路径</h1><p>首先，在AOE网中，从始点到终点<strong>具有最大路径长度</strong>（该路径上的各个活动所持续的时间之和）的路径为关键路径。</p><p>计算关键路径，只需求出上面的四个特征属性，然后取e(i)&#x3D;l(i)的边即为关键路径上的边（关键路径可能不止一条）。</p><p>先来看看四个特征属性的含义：</p><p>Ø Ve(j)：是指从始点开始到顶点Vk的最大路径长度</p><p>计算技巧：</p><p><strong>(1)从前向后，取大值：直接前驱结点的Ｖe(j)＋到达边（指向顶点的边）的权值，有多个值的取较大者</strong></p><p><strong>(2)首结点Ve(j)已知，为0</strong></p><p>如上图各顶点（事件）的Ve(j)： （从V1开始）</p><p><img src="//img-blog.csdn.net/20131101084153593?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2FuZzM3OTI3NTYxNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p><p>Ø  Vl(j)：在不推迟整个工期的前提下，事件vk允许的最晚发生时间</p><p><strong>计算技巧：</strong></p><p>(1)从后向前，取小值：直接后继结点的Ｖl(j) –发出边（从顶点发出的边）的权值，有多个值的取较小者;</p><p>(2)终结点Vl(j)已知，等于它的Ve(j)）</p><p>如上图各顶点（事件）的Vl(j)： （从V7开始,它的最早、最晚发生时间相同,都为10）：</p><p><img src="//img-blog.csdn.net/20131101084157578?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2FuZzM3OTI3NTYxNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p><p>Ø  e(i): 若活动ai由弧&lt;vk,vj&gt;表示，则活动ai的最早开始时间应该等于事件vk的最早发生时间。因而，有：e[i]&#x3D;ve[k];（即：边（活动）的最早开始时间等于，它的<strong>发出</strong>顶点的<strong>最早发生</strong>时间）</p><p>如上图各边（活动）的e(i)：</p><p><img src="//img-blog.csdn.net/20131101084200953?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2FuZzM3OTI3NTYxNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p><p>Ø  l(i): 若活动ai由弧&lt;vk,vj&gt;表示，则ai的最晚开始时间要保证事件vj的最迟发生时间不拖后。 因而有：l[i]&#x3D;vl[j]-len&lt;vk,vj&gt;1（为边（活动）的<strong>到达</strong>顶点的<strong>最晚发生</strong>时间减去<strong>边的权值</strong>）</p><p>如上图各边（活动）的l(i)：</p><p><img src="//img-blog.csdn.net/20131101084204906?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2FuZzM3OTI3NTYxNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p><p>至此已介绍完了四个特征属性的求法，也求出了上图中边的e(i)和l(i),取出e(i)&#x3D;l(i)的边为a1、a2、a4、a8、a9，即为关键路径上的边，所以关键路径有两条：a1 a4 a9和 a2 a8 a9</p><p><img src="//img-blog.csdn.net/20131101084208687?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2FuZzM3OTI3NTYxNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>求关键路径，只需理解顶点（事件）和边（活动）各自的两个特征属性以及求法即可：</p><p>Ø  先根据首结点的Ve(j)&#x3D;0<strong>由前向后</strong>计算各顶点的最早发生时间</p><p>Ø  再根据终结点的Vl(j)等于它的Ve(j)<strong>由后向前</strong>依次求解各顶点的最晚发生时间</p><p>Ø  根据边的e(i)等于它的发出顶点的Ve(j)计算各边的最早开始时间（最早开始，对应最早发生）</p><p>Ø  根据边的l(i)等于它的到达顶点的Vl(j)减去边的权值计算各边的最晚开始时间（最晚开始，对应最晚发生）</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/suncoolcat/&quot;&gt;滴水穿石&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;教你轻松计算AOE网关键路径&quot;&gt;&lt;a href=&quot;#教你轻松计算AOE网关键路径&quot; class=&quot;headerlink&quot; title=&quot;教你</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Scrapy-入门</title>
    <link href="http://example.com/2022/05/06/Scrapy-%E5%85%A5%E9%97%A8/"/>
    <id>http://example.com/2022/05/06/Scrapy-%E5%85%A5%E9%97%A8/</id>
    <published>2022-05-06T13:40:09.000Z</published>
    <updated>2022-05-06T13:40:14.011Z</updated>
    
    <content type="html"><![CDATA[<h1 id="入门教程"><a href="#入门教程" class="headerlink" title="入门教程"></a>入门教程</h1><h4 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h4><p>完成下列任务:</p><ul><li><p>创建一个Scrapy项目</p></li><li><p>定义提取的结构化数据(Item)</p></li><li><p>编写爬取网站的 spider 并提取出结构化数据(Item)</p></li><li><p>编写 Item Pipeline 来存储提取到的Item(即结构化数据)</p></li><li></li></ul><h3 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h3><p>在开始爬取之前，您必须创建一个新的<code>Scrapy</code>项目。 进入您打算存储代码的目录中，运行下列命令:</p><p> 复制代码</p><ol><li><code>scrapy startproject tutorial</code></li></ol><p>运行过程：</p><p><img src="https://static.sitestack.cn/projects/piaosanlang-spiders/b0939eadc6fdbb910806f8ccb68db4ad.gif" alt="入门教程 - 图1"></p><p>该命令将会创建包含下列内容的 tutorial 目录:</p><p>这些文件分别是:</p><p> 复制代码</p><ol><li><code>scrapy.cfg: 项目的配置文件；（用于发布到服务器）</code></li><li><code>tutorial/: 该项目文件夹。之后将在此编写Python代码。</code></li><li><code>tutorial/items.py: 项目中的item文件;（定义结构化数据字段field）.</code></li><li><code>tutorial/pipelines.py: 项目中的pipelines文件;（用于存放执行后期数据处理的功能，定义如何存储结构化数据)</code></li><li><code>tutorial/settings.py: 项目的设置文件；(如何修改User-Agent，设置爬取时间间隔，设置代理，配置中间件等等)</code></li><li><code>tutorial/spiders/: 放置spider代码的目录;（编写爬取网站规则）</code></li></ol><h3 id="定义Item"><a href="#定义Item" class="headerlink" title="定义Item"></a>定义Item</h3><p>Item 定义结构化数据字段，用来保存爬取到的数据；其使用方法和python字典类似</p><p>可以通过创建一个 <code>scrapy.Item</code> 类， 并且定义类型为 <code>scrapy.Field</code>的类属性来定义一个Item。</p><p>首先根据需要从<a href="http://hr.tencent.com/position.php?&start=0#a">腾讯招聘</a>获取到的数据对item进行建模。 我们需要从<code>腾讯招聘</code>中获取 职位名称、<code>职位详情页url</code>、职位类别、人数、工作地点以及发布时间。 对此，在item中定义相应的字段。编辑 <code>tutorial</code> 目录中的 <code>items.py</code> 文件:</p><p> 复制代码</p><ol><li><p><code>import scrapy</code></p></li><li><p><code>class RecruitItem(scrapy.Item):</code></p></li><li><p><code>name = scrapy.Field()</code></p></li><li><p><code>detailLink = scrapy.Field()</code></p></li><li><p><code>catalog = scrapy.Field()</code></p></li><li><p><code>recruitNumber = scrapy.Field()</code></p></li><li><p><code>workLocation = scrapy.Field()</code></p></li><li><p><code>publishTime = scrapy.Field()</code></p></li></ol><h3 id="编写第一个爬虫-Spider"><a href="#编写第一个爬虫-Spider" class="headerlink" title="编写第一个爬虫(Spider)"></a>编写第一个爬虫(Spider)</h3><p>Spider是开发者编写用于从单个网站(或者一些网站)爬取数据的类。</p><p>创建一个Spider，必须继承 ‘scrapy.Spider’ 类， 需要定义以下三个属性:</p><ul><li><p>name:</p><p>  spider名字；必须是唯一的</p></li><li><p>start_urls:</p><p>  初始的URL列表</p></li><li><p>parse(self, response)：</p><p>  每个初始URL完成下载后被调用</p><p>  这个函数要完成的功能：</p></li></ul><p> 复制代码</p><ol><li><code>1.负责解析返回的网页数据(response.body)，提取结构化数据(生成item)</code></li><li><code>2.生成需要下一页的请求URL。</code></li></ol><p>以下为我们的第一个Spider代码，保存在 tutorial&#x2F;spiders 目录下的 tencent_spider.py 文件中:</p><p> 复制代码</p><ol><li><p><code>import scrapy</code></p></li><li><p><code>class RecruitSpider(scrapy.spiders.Spider):</code></p></li><li><p><code>name = &quot;tencent&quot;</code></p></li><li><p><code>allowed_domains = [&quot;hr.tencent.com&quot;]</code></p></li><li><p><code>start_urls = [</code></p></li><li><p><code>&quot;http://hr.tencent.com/position.php?&amp;start=0#a&quot;</code></p></li><li><p><code>]</code></p></li><li><p><code>def parse(self, response):</code></p></li><li><p><code>f = open(&#39;tengxun.txt&#39;, &#39;wb&#39;)</code></p></li><li><p><code>f.write(response.body)</code></p></li><li><p><code>f.close()</code></p></li></ol><h3 id="爬取"><a href="#爬取" class="headerlink" title="爬取"></a>爬取</h3><p>进入项目的根目录，执行下列命令启动spider:</p><p> 复制代码</p><ol><li><code>scrapy crawl tencent</code></li></ol><p>crawl tencent 启动用于爬取 tencent 的spider，您将得到类似的输出:</p><p><img src="https://static.sitestack.cn/projects/piaosanlang-spiders/919526005b0a52806c14376101fa97cd.gif" alt="入门教程 - 图2"></p><p>现在，查看当前目录，会注意到有文件被创建了: tengxun.txt,正如我们的 parse 方法里做的一样。</p><p><strong>注意，在刚启动的时候会有一段error信息，不用理会</strong></p><p>在第六天作业里面有说明原因</p><p> 复制代码</p><ol><li><code>2016-08-11 13:07:35 [boto] ERROR: Caught exception reading instance data</code></li><li><code>Traceback (most recent call last):</code></li><li><code>File &quot;/usr/lib/python2.7/dist-packages/boto/utils.py&quot;, line 210, in retry_url</code></li><li><code>r = opener.open(req, timeout=timeout)</code></li><li><code>File &quot;/usr/lib/python2.7/urllib2.py&quot;, line 429, in open</code></li><li><code>response = self._open(req, data)</code></li><li><code>File &quot;/usr/lib/python2.7/urllib2.py&quot;, line 447, in _open</code></li><li><code>&#39;_open&#39;, req)</code></li><li><code>File &quot;/usr/lib/python2.7/urllib2.py&quot;, line 407, in _call_chain</code></li><li><code>result = func(*args)</code></li><li><code>File &quot;/usr/lib/python2.7/urllib2.py&quot;, line 1228, in http_open</code></li><li><code>return self.do_open(httplib.HTTPConnection, req)</code></li><li><code>File &quot;/usr/lib/python2.7/urllib2.py&quot;, line 1198, in do_open</code></li><li><code>raise URLError(err)</code></li><li><code>URLError: &lt;urlopen error timed out&gt;</code></li></ol><h3 id="刚才发生了什么？"><a href="#刚才发生了什么？" class="headerlink" title="刚才发生了什么？"></a>刚才发生了什么？</h3><p>Scrapy为Spider的 start_urls 属性中的每个URL创建了 <code>scrapy.Request</code> 对象，并将 parse 方法作为回调函数(callback)赋值给了Request。</p><p>Request对象经过调度，执行生成 <code>scrapy.http.Response</code> 对象并送回给<code>parse()</code> 方法。</p><h3 id="提取Item"><a href="#提取Item" class="headerlink" title="提取Item"></a>提取Item</h3><h4 id="Selectors选择器简介"><a href="#Selectors选择器简介" class="headerlink" title="Selectors选择器简介"></a>Selectors选择器简介</h4><p><code>Scrapy Selectors</code> 内置<code>XPath</code> 和 <code>CSS Selector</code> 表达式机制</p><p>XPath表达式的例子及对应的含义:</p><p> 复制代码</p><ol><li><code>/html/head/title: 选择&lt;HTML&gt;文档中 &lt;head&gt; 标签内的 &lt;title&gt; 元素</code></li><li><code>/html/head/title/text(): 选择上面提到的 &lt;title&gt; 元素的文字</code></li><li><code>//td: 选择所有的 &lt;td&gt; 元素</code></li><li><code>//div[@class=&quot;mine&quot;]: 选择所有具有 class=&quot;mine&quot; 属性的 div 元素</code></li></ol><p>Selector有四个基本的方法:</p><p> 复制代码</p><ol><li><code>xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表 。</code></li><li><code>css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表.</code></li><li><code>extract(): 序列化该节点为unicode字符串并返回list。</code></li><li><code>re(): 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。</code></li></ol><h3 id="尝试Selector选择器"><a href="#尝试Selector选择器" class="headerlink" title="尝试Selector选择器"></a>尝试Selector选择器</h3><p>为了介绍Selector的使用方法，接下来我们将要使用内置的 scrapy shell 。Scrapy Shell需要您预装好IPython(一个扩展的Python终端)。</p><p>您需要进入项目的根目录，执行下列命令来启动shell:</p><p> 复制代码</p><ol><li><code>scrapy shell &quot;http://hr.tencent.com/position.php?&amp;start=0#a&quot;</code></li></ol><p>注解:当您在终端运行Scrapy时，请一定记得给url地址加上引号，否则包含参数的url(例如 &amp; 字符)会导致Scrapy运行失败。</p><p>shell的输出类似:</p><p><img src="https://static.sitestack.cn/projects/piaosanlang-spiders/2a926bfd0a51273e640617660ec14b76.gif" alt="入门教程 - 图3"></p><p>当shell载入后，将得到一个包含response数据的本地 <code>response</code> 变量。输入 <code>response.body</code>将输出response的包体， 输出 <code>response.headers</code> 可以看到response的包头。</p><ul><li>当输入 <code>response.selector</code> 时， 将获取到一个response 初始化的类 <code>Selector</code> 的对象</li><li>此时，可以通过使用 response.selector.xpath() 或 response.selector.css() 来对 response 进行查询。</li><li>或者，scrapy也对 response.selector.xpath() 及 response.selector.css() 提供了一些快捷方式, 例如 response.xpath() 或 response.css()</li></ul><p>让我们来试试:</p><p> 复制代码</p><ol><li><p><code>response.xpath(&#39;//title&#39;)</code></p></li><li><p><code>[&lt;Selector xpath=&#39;//title&#39; data=u&#39;&lt;title&gt;\u804c\u4f4d\u641c\u7d22 | \u793e\u4f1a\u62db\u8058 | Tencent \u817e\u8baf\u62db\u8058&lt;/title&#39;&gt;]</code></p></li><li><p><code>response.xpath(&#39;//title&#39;).extract()</code></p></li><li><p><code>[u&#39;&lt;title&gt;\u804c\u4f4d\u641c\u7d22 | \u793e\u4f1a\u62db\u8058 | Tencent \u817e\u8baf\u62db\u8058&lt;/title&gt;&#39;]</code></p></li><li><p><code>print response.xpath(&#39;//title&#39;).extract()[0]</code></p></li><li><p><code>&lt;title&gt;职位搜索 | 社会招聘 | Tencent 腾讯招聘&lt;/title&gt;</code></p></li><li><p><code>response.xpath(&#39;//title/text()&#39;)</code></p></li><li><p><code>&lt;Selector xpath=&#39;//title/text()&#39; data=u&#39;\u804c\u4f4d\u641c\u7d22 | \u793e\u4f1a\u62db\u8058 | Tencent \u817e\u8baf\u62db\u8058&#39;&gt;</code></p></li><li><p><code>response.xpath(&#39;//title/text()&#39;)[0].extract()</code></p></li><li><p><code>u&#39;\u804c\u4f4d\u641c\u7d22 | \u793e\u4f1a\u62db\u8058 | Tencent \u817e\u8baf\u62db\u8058&#39;</code></p></li><li><p><code>print response.xpath(&#39;//title/text()&#39;)[0].extract()</code></p></li><li><p><code>职位搜索 | 社会招聘 | Tencent 腾讯招聘</code></p></li><li><p><code>response.xpath(&#39;//title/text()&#39;).re(&#39;(\w+):&#39;)</code></p></li><li><p><code>[u&#39;\u804c\u4f4d\u641c\u7d22&#39;,</code></p></li><li><p><code>u&#39;\u793e\u4f1a\u62db\u8058&#39;,</code></p></li><li><p><code>u&#39;Tencent&#39;,</code></p></li><li><p><code>u&#39;\u817e\u8baf\u62db\u8058&#39;]</code></p></li></ol><h3 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a>提取数据</h3><p>现在，我们来尝试从这些页面中提取些有用的数据。</p><p>我们可以通过XPath选择该页面中网站列表里所有 <code>lass=even</code> 元素:</p><p> 复制代码</p><ol><li><code>site = response.xpath(&#39;//*[@class=&quot;even&quot;]&#39;)</code></li></ol><p>职位名称:</p><p> 复制代码</p><ol><li><code>print site[0].xpath(&#39;./td[1]/a/text()&#39;).extract()[0]</code></li><li><code>TEG15-运营开发工程师（深圳）</code></li></ol><p>职位名称详情页:</p><p> 复制代码</p><ol><li><code>print site[0].xpath(&#39;./td[1]/a/@href&#39;).extract()[0]</code></li><li><code>position_detail.php?id=20744&amp;keywords=&amp;tid=0&amp;lid=0</code></li></ol><p>职位类别:</p><p> 复制代码</p><ol><li><code>print site[0].xpath(&#39;./td[2]/text()&#39;).extract()[0]</code></li><li><code>技术类</code></li></ol><p>对于 <code>.xpath()</code> 调用返回<code>selector</code>组成的<code>list</code>，因此可以拼接更多的 .xpath() 来进一步获取某个节点。</p><p> 复制代码</p><ol><li><code>for sel in response.xpath(&#39;//*[@class=&quot;even&quot;]&#39;):</code></li><li><code>name = sel.xpath(&#39;./td[1]/a/text()&#39;).extract()[0]</code></li><li><code>detailLink = sel.xpath(&#39;./td[1]/a/@href&#39;).extract()[0]</code></li><li><code>catalog = sel.xpath(&#39;./td[2]/text()&#39;).extract()[0]</code></li><li><code>recruitNumber = sel.xpath(&#39;./td[3]/text()&#39;).extract()[0]</code></li><li><code>workLocation = sel.xpath(&#39;./td[4]/text()&#39;).extract()[0]</code></li><li><code>publishTime = sel.xpath(&#39;./td[5]/text()&#39;).extract()[0]</code></li><li><code>print name, detailLink, catalog,recruitNumber,workLocation,publishTime</code></li></ol><p>在我们的<code>tencent_spider.py</code>文件修改成如下代码:</p><p> 复制代码</p><ol><li><p><code>import scrapy</code></p></li><li><p><code>class RecruitSpider(scrapy.spiders.Spider):</code></p></li><li><p><code>name = &quot;tencent&quot;</code></p></li><li><p><code>allowed_domains = [&quot;hr.tencent.com&quot;]</code></p></li><li><p><code>start_urls = [</code></p></li><li><p><code>&quot;http://hr.tencent.com/position.php?&amp;start=0#a&quot;</code></p></li><li><p><code>]</code></p></li><li><p><code>def parse(self, response):</code></p></li><li><p><code>for sel in response.xpath(&#39;//*[@class=&quot;even&quot;]&#39;):</code></p></li><li><p><code>name = sel.xpath(&#39;./td[1]/a/text()&#39;).extract()[0]</code></p></li><li><p><code>detailLink = sel.xpath(&#39;./td[1]/a/@href&#39;).extract()[0]</code></p></li><li><p><code>catalog = sel.xpath(&#39;./td[2]/text()&#39;).extract()[0]</code></p></li><li><p><code>recruitNumber = sel.xpath(&#39;./td[3]/text()&#39;).extract()[0]</code></p></li><li><p><code>workLocation = sel.xpath(&#39;./td[4]/text()&#39;).extract()[0]</code></p></li><li><p><code>publishTime = sel.xpath(&#39;./td[5]/text()&#39;).extract()[0]</code></p></li><li><p><code>print name, detailLink, catalog,recruitNumber,workLocation,publishTime</code></p></li></ol><p>如图所示：</p><p><img src="https://static.sitestack.cn/projects/piaosanlang-spiders/10efeb366ce0b9a924778f8269880ba5.png" alt="入门教程 - 图4"></p><p>现在尝试再次爬取<code>hr.tencent.com</code>，您将看到爬取到的网站信息被成功输出:</p><p> 复制代码</p><ol><li><code>scrapy crawl tencent</code></li></ol><p>运行过程：</p><p><img src="https://static.sitestack.cn/projects/piaosanlang-spiders/f8605ad6da5b180c4ba5bd0f82c0e16d.gif" alt="入门教程 - 图5"></p><h3 id="使用item"><a href="#使用item" class="headerlink" title="使用item"></a>使用item</h3><p>Item 对象是自定义的python字典。可以使用标准的字典语法来获取到其每个字段的值。</p><p>输入 `scrapy shell’</p><p> 复制代码</p><ol><li><p><code>import scrapy</code></p></li><li><p><code>class RecruitItem(scrapy.Item):</code></p></li><li><p><code>name = scrapy.Field()</code></p></li><li><p><code>detailLink = scrapy.Field()</code></p></li><li><p><code>catalog = scrapy.Field()</code></p></li><li><p><code>recruitNumber = scrapy.Field()</code></p></li><li><p><code>workLocation = scrapy.Field()</code></p></li><li><p><code>publishTime = scrapy.Field()</code></p></li><li><p><code>item = RecruitItem()</code></p></li><li><p><code>item[&#39;name&#39;] = &#39;sanlang&#39;</code></p></li><li><p><code>item[&#39;name&#39;]</code></p></li><li><p><code>&#39;sanlang&#39;</code></p></li></ol><p>一般来说，Spider将会将爬取到的数据以Item对象返回。所以为了将爬取的数据返回，最终<code>tencent_spider.py</code>代码将是:</p><p> 复制代码</p><ol><li><p><code>import scrapy</code></p></li><li><p><code>from tutorial.items import RecruitItem</code></p></li><li><p><code>class RecruitSpider(scrapy.spiders.Spider):</code></p></li><li><p><code>name = &quot;tencent&quot;</code></p></li><li><p><code>allowed_domains = [&quot;hr.tencent.com&quot;]</code></p></li><li><p><code>start_urls = [</code></p></li><li><p><code>&quot;http://hr.tencent.com/position.php?&amp;start=0#a&quot;</code></p></li><li><p><code>]</code></p></li><li><p><code>def parse(self, response):</code></p></li><li><p><code>for sel in response.xpath(&#39;//*[@class=&quot;even&quot;]&#39;):</code></p></li><li><p><code>name = sel.xpath(&#39;./td[1]/a/text()&#39;).extract()[0]</code></p></li><li><p><code>detailLink = sel.xpath(&#39;./td[1]/a/@href&#39;).extract()[0]</code></p></li><li><p><code>catalog = sel.xpath(&#39;./td[2]/text()&#39;).extract()[0]</code></p></li><li><p><code>recruitNumber = sel.xpath(&#39;./td[3]/text()&#39;).extract()[0]</code></p></li><li><p><code>workLocation = sel.xpath(&#39;./td[4]/text()&#39;).extract()[0]</code></p></li><li><p><code>publishTime = sel.xpath(&#39;./td[5]/text()&#39;).extract()[0]</code></p></li><li><p><code>print name, detailLink, catalog,recruitNumber,workLocation,publishTime</code></p></li><li><p><code>item = RecruitItem()</code></p></li><li><p><code>item[&#39;name&#39;]=name.encode(&#39;utf-8&#39;)</code></p></li><li><p><code>item[&#39;detailLink&#39;]=detailLink.encode(&#39;utf-8&#39;)</code></p></li><li><p><code>item[&#39;catalog&#39;]=catalog.encode(&#39;utf-8&#39;)</code></p></li><li><p><code>item[&#39;recruitNumber&#39;]=recruitNumber.encode(&#39;utf-8&#39;)</code></p></li><li><p><code>item[&#39;workLocation&#39;]=workLocation.encode(&#39;utf-8&#39;)</code></p></li><li><p><code>item[&#39;publishTime&#39;]=publishTime.encode(&#39;utf-8&#39;)</code></p></li><li><p><code>yield item</code></p></li></ol><p>现在对<code>hr.tencent.com</code>进行爬取将会产生 RecruitItem 对象:</p><p>运行过程：</p><p><img src="https://static.sitestack.cn/projects/piaosanlang-spiders/0dc4e83404d6b04c66252fcff2123a97.gif" alt="入门教程 - 图6"></p><h3 id="保存爬取到的数据"><a href="#保存爬取到的数据" class="headerlink" title="保存爬取到的数据"></a>保存爬取到的数据</h3><p>最简单存储爬取的数据的方式是使用 <code>Feed exports</code>:</p><p> 复制代码</p><ol><li><code>scrapy crawl tencent -o items.json</code></li></ol><p>该命令将采用 JSON 格式对爬取的数据进行序列化，生成 items.json 文件。</p><p>如果需要对爬取到的item做更多更为复杂的操作，您可以编写 Item Pipeline 。 类似于我们在创建项目时对Item做的，用于您编写自己的 tutorial&#x2F;pipelines.py 也被创建。 不过如果您仅仅想要保存item，您不需要实现任何的pipeline。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;入门教程&quot;&gt;&lt;a href=&quot;#入门教程&quot; class=&quot;headerlink&quot; title=&quot;入门教程&quot;&gt;&lt;/a&gt;入门教程&lt;/h1&gt;&lt;h4 id=&quot;学习目标&quot;&gt;&lt;a href=&quot;#学习目标&quot; class=&quot;headerlink&quot; title=&quot;学习目标&quot;&gt;&lt;/a</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Regular Expression</title>
    <link href="http://example.com/2022/05/06/Regular-Expression/"/>
    <id>http://example.com/2022/05/06/Regular-Expression/</id>
    <published>2022-05-06T12:55:53.000Z</published>
    <updated>2022-05-06T13:20:16.139Z</updated>
    
    <content type="html"><![CDATA[<h1 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h1><p><strong>掌握了XPath、CSS选择器，为什么还要学习正则？</strong></p><p>正则表达式，用标准正则解析，一般会把HTML当做普通文本，用指定格式匹配当相关文本，适合小片段文本，或者某一串字符(比如电话号码、邮箱账户)，或者HTML包含javascript的代码，无法用CSS选择器或者XPath</p><p><a href="http://tool.oschina.net/regex/">在线正则表达式测试网站</a></p><p><a href="https://docs.python.org/2/library/re.html#regular-expression-objects">官方文档</a></p><p><strong>了解正则表达式</strong></p><p>正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个”规则字符串”，这个”规则字符串”用来表达对字符串的一种过滤逻辑。</p><h3 id="正则表达式常见概念"><a href="#正则表达式常见概念" class="headerlink" title="正则表达式常见概念"></a>正则表达式常见概念</h3><ul><li><p>边界匹配</p><p>  ^ — 与字符串开始的地方匹配，不匹配任何字符；</p><p>  $ — 与字符串结束的地方匹配，不匹配任何字符；</p></li></ul><p> 复制代码</p><ol><li><code>str = &quot;cat abdcatdetf ios&quot;</code></li><li><code>^cat : 验证该行以c开头紧接着是a，然后是t</code></li><li><code>ios$ : 验证该行以t结尾倒数第二个字符为a倒数第三个字符为c</code></li><li><code>^cat$: 以c开头接着是a-&gt;t然后是行结束：只有cat三个字母的数据行</code></li><li><code>^$ : 开头之后马上结束：空白行，不包括任何字符</code></li><li><code>^ : 行的开头，可以匹配任何行，因为每个行都有行开头</code></li></ol><p>\b — 匹配一个单词边界，也就是单词和空格之间的位置，不匹配任何字符；</p><p> 复制代码</p><ol><li><code>&quot;er\b&quot;可以匹配&quot;never&quot;中的&quot;er&quot;，但不能匹配&quot;verb&quot;中的&quot;er&quot;。</code></li></ol><p>\B — \b取非，即匹配一个非单词边界；</p><p> 复制代码</p><ol><li><code>&quot;er\B&quot;能匹配&quot;verb&quot;中的&quot;er&quot;，但不能匹配&quot;never&quot;中的&quot;er&quot;。</code></li></ol><ul><li><p>数量词的贪婪模式与非贪婪模式</p><p>  正则表达式通常用于在文本中查找匹配的字符串。Python里数量词默认是贪婪的（在少数语言里也可能是默认非贪婪），总是尝试匹配尽可能多的字符；非贪婪的则相反，总是尝试匹配尽可能少的字符。例如：</p></li></ul><p> 复制代码</p><ol><li><code>正则表达式&quot;ab*&quot;如果用于查找&quot;abbbc&quot;，将找到&quot;abbb&quot;。而如果使用非贪婪的数量词&quot;ab*?&quot;，将找到&quot;a&quot;。</code></li></ol><ul><li>反斜杠问题</li></ul><p>与大多数编程语言相同，正则表达式里使用”\“作为转义字符，这就可能造成反斜杠困扰。</p><p>假如你需要匹配文本中的字符”\“，那么使用编程语言表示的正则表达式里将需要4个反斜杠”\\“：前两个和后两个分别用于在编程语言里转义成反斜杠，转换成两个反斜杠后再在正则表达式里转义成一个反斜杠。</p><p>Python里的原生字符串很好地解决了这个问题，这个例子中的正则表达式可以使用r”\“表示。</p><p>同样，匹配一个数字的”\d”可以写成r”\d”。有了原生字符串，你再也不用担心是不是漏写了反斜杠，写出来的表达式也更直观。</p><p> 复制代码</p><ol><li><code>import re</code></li><li><code>a=re.search(r&quot;\\&quot;,&quot;ab123bb\c&quot;)</code></li><li><code>print a.group()</code></li><li><code>\</code></li><li><code>a=re.search(r&quot;\d&quot;,&quot;ab123bb\c&quot;)</code></li><li><code>print a.group()</code></li><li><code>1</code></li></ol><h3 id="Python-Re模块"><a href="#Python-Re模块" class="headerlink" title="Python Re模块"></a>Python Re模块</h3><p>Python 自带了re模块，它提供了对正则表达式的支持。</p><h3 id="match函数"><a href="#match函数" class="headerlink" title="match函数"></a>match函数</h3><p>re.match 尝试从字符串的<strong>起始位置</strong>匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none。</p><p>下面是此函数的语法：</p><p> 复制代码</p><ol><li><code>re.match(pattern, string, flags=0)</code></li></ol><p>这里的参数的说明：</p><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>pattern</td><td>这是正则表达式来进行匹配。</td></tr><tr><td>string</td><td>这是字符串，这将被搜索匹配的模式，在字符串的开头。</td></tr><tr><td>flags</td><td>标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。</td></tr></tbody></table><p>匹配成功re.match方法返回一个匹配的对象，否则返回None。</p><p>我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。</p><table><thead><tr><th>匹配对象的方法</th><th>描述</th></tr></thead><tbody><tr><td>group(num&#x3D;0)</td><td>此方法返回整个匹配（或指定分组num）</td></tr><tr><td>groups()</td><td>此方法返回所有元组匹配的子组（空，如果没有）</td></tr></tbody></table><h3 id="例子："><a href="#例子：" class="headerlink" title="例子："></a>例子：</h3><p> 复制代码</p><ol><li><p><code>#!/usr/bin/python</code></p></li><li><p><code>import re</code></p></li><li><p><code>line = &quot;Cats are smarter than dogs&quot;</code></p></li><li><p><code>matchObj = re.match( r&#39;(.*) are (.*?) .*&#39;, line, re.M|re.I)</code></p></li><li><p><code>if matchObj:</code></p></li><li><p><code>print &quot;matchObj.group() : &quot;, matchObj.group()</code></p></li><li><p><code>print &quot;matchObj.group(1) : &quot;, matchObj.group(1)</code></p></li><li><p><code>print &quot;matchObj.group(2) : &quot;, matchObj.group(2)</code></p></li><li><p><code>else:</code></p></li><li><p><code>print &quot;No match!!&quot;</code></p></li></ol><p>当执行上面的代码，它产生以下结果：</p><p> 复制代码</p><ol><li><code>matchObj.group() : Cats are smarter than dogs</code></li><li><code>matchObj.group(1) : Cats</code></li><li><code>matchObj.group(2) : smarter</code></li></ol><h4 id="正则表达式修饰符-选项标志"><a href="#正则表达式修饰符-选项标志" class="headerlink" title="正则表达式修饰符 - 选项标志"></a>正则表达式修饰符 - 选项标志</h4><p>正则表达式字面可以包含一个可选的修饰符来控制匹配的各个方面。修饰符被指定为一个可选的标志。可以使用异或提供多个修饰符（|），如先前所示，并且可以由这些中的一个来表示：</p><table><thead><tr><th>修饰符</th><th>描述</th></tr></thead><tbody><tr><td>re.I(re.IGNORECASE)</td><td>使匹配对大小写不敏感</td></tr><tr><td>re.M(MULTILINE)</td><td>多行匹配，影响 ^ 和 $</td></tr><tr><td>re.S(DOTALL)</td><td>使 . 匹配包括换行在内的所有字符</td></tr><tr><td>re.X(VERBOSE)</td><td>正则表达式可以是多行，忽略空白字符，并可以加入注释</td></tr></tbody></table><h3 id="findall-函数"><a href="#findall-函数" class="headerlink" title="findall()函数"></a>findall()函数</h3><p>re.findall(pattern, string, flags&#x3D;0)</p><p>返回字符串中所有模式的非重叠的匹配，作为字符串列表。该字符串扫描左到右，并匹配返回的顺序发现</p><p> 复制代码</p><ol><li><p><code>默认：</code></p></li><li><p><code>pattren = &quot;\w+&quot;</code></p></li><li><p><code>target = &quot;hello world\nWORLD HELLO&quot;</code></p></li><li><p><code>re.findall(pattren,target)</code></p></li><li><p><code>[&#39;hello&#39;, &#39;world&#39;, &#39;WORLD&#39;, &#39;HELLO&#39;]</code></p></li><li><p><code>re.I:</code> </p></li><li><p><code>re.findall(&quot;world&quot;, target,re.I)</code></p></li><li><p><code>[&#39;world&#39;, &#39;WORLD&#39;]</code></p></li><li><p><code>re.S:</code> </p></li><li><p><code>re.findall(&quot;world.WORLD&quot;, target,re.S)</code></p></li><li><p><code>[&quot;world\nworld&quot;]</code></p></li><li><p><code>re.findall(&quot;hello.*WORLD&quot;, target,re.S)</code></p></li><li><p><code>[&#39;hello world\nWORLD&#39;]</code></p></li><li><p><code>re.M:</code></p></li><li><p><code>re.findall(&quot;^WORLD&quot;,target,re.M)</code></p></li><li><p><code>[&quot;WORLD&quot;]</code></p></li><li><p><code>re.X:</code></p></li><li><p><code>reStr = &#39;&#39;&#39;\d&#123;3&#125;  #区号</code></p></li><li><p><code>-\d&#123;8&#125;&#39;&#39;&#39; #号码</code></p></li><li><p><code>re.findall(reStr,&quot;010-12345678&quot;,re.X)</code> </p></li><li><p><code>[&quot;010-12345678&quot;]</code></p></li></ol><h3 id="search函数"><a href="#search函数" class="headerlink" title="search函数"></a>search函数</h3><p>re.search 扫描整个字符串并返回第一个成功的匹配。</p><p>下面是此函数语法：</p><p> 复制代码</p><ol><li><code>re.search(pattern, string, flags=0)</code></li></ol><p>这里的参数说明：</p><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>pattern</td><td>这是正则表达式来进行匹配。</td></tr><tr><td>string</td><td>这是字符串，这将被搜索到的字符串中的任何位置匹配的模式。</td></tr><tr><td>flags</td><td>标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。</td></tr></tbody></table><p>匹配成功re.search方法返回一个匹配的对象，否则返回None。</p><p>我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。</p><table><thead><tr><th>匹配对象的方法</th><th>描述</th></tr></thead><tbody><tr><td>group(num&#x3D;0)</td><td>此方法返回整个匹配（或指定分组num）</td></tr><tr><td>groups()</td><td>此方法返回所有元组匹配的子组（空，如果没有）</td></tr></tbody></table><h4 id="例子：-1"><a href="#例子：-1" class="headerlink" title="例子："></a>例子：</h4><p> 复制代码</p><ol><li><p><code>#!/usr/bin/python</code></p></li><li><p><code>import re</code></p></li><li><p><code>line = &quot;Cats are smarter than dogs&quot;;</code></p></li><li><p><code>searchObj = re.search( r&#39;(.*) are (.*?) .*&#39;, line, re.M|re.I)</code></p></li><li><p><code>if searchObj:</code></p></li><li><p><code>print &quot;searchObj.group() : &quot;, searchObj.group()</code></p></li><li><p><code>print &quot;searchObj.group(1) : &quot;, searchObj.group(1)</code></p></li><li><p><code>print &quot;searchObj.group(2) : &quot;, searchObj.group(2)</code></p></li><li><p><code>else:</code></p></li><li><p><code>print &quot;Nothing found!!&quot;</code></p></li></ol><p>当执行上面的代码，它产生以下结果：</p><p> 复制代码</p><ol><li><code>matchObj.group() : Cats are smarter than dogs</code></li><li><code>matchObj.group(1) : Cats</code></li><li><code>matchObj.group(2) : smarter</code></li></ol><h4 id="re-match与re-search的区别"><a href="#re-match与re-search的区别" class="headerlink" title="re.match与re.search的区别"></a>re.match与re.search的区别</h4><p>re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。</p><h4 id="例子：-2"><a href="#例子：-2" class="headerlink" title="例子："></a>例子：</h4><p> 复制代码</p><ol><li><p><code>#!/usr/bin/python</code></p></li><li><p><code>import re</code></p></li><li><p><code>line = &quot;Cats are smarter than dogs&quot;;</code></p></li><li><p><code>matchObj = re.match( r&#39;dogs&#39;, line, re.M|re.I)</code></p></li><li><p><code>if matchObj:</code></p></li><li><p><code>print &quot;match --&gt; matchObj.group() : &quot;, matchObj.group()</code></p></li><li><p><code>else:</code></p></li><li><p><code>print &quot;No match!!&quot;</code></p></li><li><p><code>searchObj = re.search( r&#39;dogs&#39;, line, re.M|re.I)</code></p></li><li><p><code>if searchObj:</code></p></li><li><p><code>print &quot;search --&gt; searchObj.group() : &quot;, searchObj.group()</code></p></li><li><p><code>else:</code></p></li><li><p><code>print &quot;Nothing found!!&quot;</code></p></li></ol><p>当执行上面的代码，产生以下结果：</p><p> 复制代码</p><ol><li><code>No match!!</code></li><li><code>search --&gt; matchObj.group() : dogs</code></li></ol><h3 id="搜索和替换"><a href="#搜索和替换" class="headerlink" title="搜索和替换"></a>搜索和替换</h3><p>Python 的re模块提供了re.sub用于替换字符串中的匹配项。</p><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><p> 复制代码</p><ol><li><code>re.sub(pattern, repl, string, max=0)</code></li></ol><p>返回的字符串是在字符串中用 RE 最左边不重复的匹配来替换。如果模式没有发现，字符将被没有改变地返回。可选参数 count 是模式匹配后替换的最大次数；count 必须是非负整数。缺省值是 0 表示替换所有的匹配。实例：</p><h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><p>下面是一个爬虫做翻页面例子：</p><p> 复制代码</p><ol><li><p><code>#!/usr/bin/python</code></p></li><li><p><code>import re</code></p></li><li><p><code>url = &quot;http://hr.tencent.com/position.php?&amp;start=10&quot;</code></p></li><li><p><code>page = re.search(&#39;start=(\d+)&#39;,url).group(1)</code></p></li><li><p><code>nexturl = re.sub(r&#39;start=(\d+)&#39;, &#39;start=&#39;+str(int(page)+10), url)</code></p></li><li><p><code>print &quot;Next Url : &quot;, nexturl</code></p></li></ol><p>当执行上面的代码，产生以下结果：</p><p> 复制代码</p><ol><li><code>Next Url : http://hr.tencent.com/position.php?&amp;start=20</code></li></ol><h3 id="正则表达式语法"><a href="#正则表达式语法" class="headerlink" title="正则表达式语法"></a>正则表达式语法</h3><p>下表列出了Python中可用正则表达式语法：</p><p><img src="https://static.sitestack.cn/projects/piaosanlang-spiders/4fdd105dc5ac24d05a59d8f215937e45.png" alt="非结构化数据之正则表达式 - 图1"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;正则表达式&quot;&gt;&lt;a href=&quot;#正则表达式&quot; class=&quot;headerlink&quot; title=&quot;正则表达式&quot;&gt;&lt;/a&gt;正则表达式&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;掌握了XPath、CSS选择器，为什么还要学习正则？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;正则表达式，用</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2022/04/29/hello-world/"/>
    <id>http://example.com/2022/04/29/hello-world/</id>
    <published>2022-04-29T14:27:05.667Z</published>
    <updated>2022-05-04T11:20:17.501Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
